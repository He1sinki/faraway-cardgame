# PPO Configuration (Phase 1.3 Draft)
policy: "MlpPolicy"
learning_rate: 0.0003
gamma: 0.995
gae_lambda: 0.95
clip_range: 0.2
entropy_coef: 0.01
value_coef: 0.5
max_grad_norm: 0.5
n_steps: 2048
batch_size: 256
n_epochs: 3
vf_clip: null
normalize_advantage: true
seed: 12345
obs_dim: 0   # à remplir après encodeur
act_dim: 208 # 2R + S + 1 = 208 (R=77, S=53)
